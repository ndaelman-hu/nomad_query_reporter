Index: backend.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import json\nimport pandas as pd\nimport requests\nimport sys\nimport time\n\n# Query NOMAD\nupload_id = \"-jbjmBQ4SNSMi9CkaXhhug\"\n\nnomad_url = \"https://nomad-lab.eu/prod/v1/api/v1/entries/archive/query\"  # ! add support for stagging\nnomad_params = {\n    \"query\": {\"upload_id\": upload_id},\n    \"pagination\": {\n            \"page_size\": 1e2,  # 1e4 is the max supported by NOMAD\n        },\n    \"required\": {\n        \"resolve-inplace\": True,\n        \"metadata\": {\n            \"datasets\": {\n                \"doi\": \"*\",\n            },\n        },\n        \"results\": {\n            \"material\": {\n                \"material_name\": \"*\",\n                \"chemical_formula_iupac\": \"*\",\n                \"symmetry\": \"*\",\n            },\n            \"method\": {\n                \"method_name\": \"*\",\n                \"workflow_name\": \"*\",\n                \"simulation\": {\n                    \"program_name\": \"*\",\n                    \"program_version\": \"*\",\n                },\n            }\n        }\n    }\n}\n\ndef get_leaf_nodes(data, path=None):\n    if path is None:\n        path = []\n    \n    if isinstance(data, dict):\n        for key, value in data.items():\n            yield from get_leaf_nodes(value, path + [key])\n    elif isinstance(data, list):\n        for i, item in enumerate(data):\n            yield from get_leaf_nodes(item, path + [i])\n    else:\n        yield path, data\n\ndef llama_complete(llama_prompt: dict[str, str]) -> dict[str, str]:\n    llama_prompt['model'] = \"llama3:70b\"\n    llama_prompt['seed'] = 42\n    llama_prompt['temperature'] = 0.05\n    llama_prompt['stream'] = False\n    return llama_prompt\n\ndef llama_inject_context(llama_prompt: dict[str, str], llama_responses: list[dict[str, str]]) -> dict[str, str]:\n    response_end = llama_responses[-1]\n    llama_prompt['context'] = response_end['context'] if response_end['done'] else []\n    return llama_prompt\n\ndef llama_response_to_list(llama_response: dict[str, str]) -> list[dict[str, str]]:\n    return json.loads(llama_response.content.decode('ascii').strip())['message']['content']\n\n# Check if the NOMAD query was successful\nnomad_df_header = []\nnomad_df_body = []\ntrial_counter, max_trials = 0, 10\nwhile True:\n    nomad_response = requests.post(nomad_url, json=nomad_params)\n    trial_counter += 1\n    if nomad_response.status_code == 200:\n        nomad_data = nomad_response.json()\n        for entry in nomad_data['data']:\n            nomad_df_body.append([value for _, value in get_leaf_nodes(entry[\"archive\"])])\n        if nomad_df_header == []:\n            nomad_df_header = [\".\".join(map(str, path)) for path, _ in get_leaf_nodes(entry[\"archive\"])]\n        if (next_page := nomad_data['pagination'].get('next_page_after_value', '')):\n            nomad_params[\"pagination\"][\"page_after_value\"] = next_page\n            trial_counter = 0 # reset the trial counter\n        else:\n            break\n    elif trial_counter >= max_trials:\n        print(f\"Failed to query NOMAD after {max_trials} trials.\")\n        sys.exit(1)\n    elif nomad_response.status_code in (502, 503):\n        print(\"Retrying query to NOMAD...\")\n        time.sleep(10)\n    else:\n        print(\"Failed to query NOMAD:\", nomad_response.text)\n        sys.exit(1)  # ! add error handling\nprint(f\"Found {len(nomad_df_body)} results in NOMAD for upload_id {upload_id}. Analyzing...\")\n\nnomad_df = pd.DataFrame(nomad_df_body, columns=nomad_df_header)\nprint(nomad_df.describe(include=\"all\"))\n\n# Push a llama query\nllama_url = \"http://172.28.105.30/backend/api/chat\"\nllama_init_params = {\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"\"\"\n                You are a data analyst, summarizing data from a database.\n                You give me short (max 250 words) responses to my prompts.\n                You stay on-topic and provide relevant information.\n                No disclaimer or suggestions for improvements.\n            \"\"\".strip().replace(\"\\n\", \"\"),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\n                Write me a Methods section for a paper draft.\n                Here is a response from the database API:\n            \"\"\".strip().replace(\"\\n\", \"\"),\n        },\n        {\n            \"role\": \"user\",\n            \"content\": str(nomad_df.describe(include=\"all\")),\n        }\n    ]  \n}\n\nllama_citation = {\n    \"prompt\": \"\"\"\n        Amend the previous response by adding citations. Say that a 'similar setup as in [1]' was used.\n        Then, at the end, add the actual citations. You can go over the previous word limit now.\n        Avoid citations of methodology, programs, or experimental hardware.\n    \"\"\".strip().replace(\"\\n\", \"\"),\n} # ! query another API for the DOI\n\nllama_init_response = requests.post(llama_url, json=llama_complete(llama_init_params))\n\n# Check if the llama query was successful\nif llama_init_response.status_code == 200:\n    print(llama_response_to_list(llama_init_response))\nelse:\n    print(\"Failed to push llama query:\", llama_init_response.text)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/backend.py b/backend.py
--- a/backend.py	(revision a8598684831d927d7264f6a81a1d80f7beb274eb)
+++ b/backend.py	(date 1715071087523)
@@ -34,7 +34,8 @@
                     "program_version": "*",
                 },
             }
-        }
+        },
+        "data": "*"
     }
 }
 
